---
layout: post
title:  "Project Documentation: Hilti SLAM Challenge"
date:   2021-09-08 13:39:00 +0530
blurb: ""
og_image: /assets/img/content/hilti/Banner.jpg
category: robotics
---

<img src="{{ "/assets/img/content/hilti/Banner.jpg" | absolute_url }}" alt="Banner image" class="post-pic" width=300/>
<br />
<br />

### Problem Definition
The goal is to estimate the position of the handheld device as accurately as possible, utilizing any desired sensor combinations, which includes images, IMU measurements, and LIDAR data recorded with a custom handheld device.


### Progress
- [Kannala-Brandt paper](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.7767&rep=rep1&type=pdf)
- [Calibrating IMU: imu_utils](https://github.com/gaowenliang/imu_utils)
- [What are fx and fy](https://stackoverflow.com/questions/16329867/why-does-the-focal-length-in-the-camera-intrinsics-matrix-have-two-dimensions)


### Preliminary Reading

- Pose Graph Optimization
    + [GTSAM](https://gtsam.org/): It uses factor graphs and Bayes networks as the underlying computing paradigm rather than sparse matrices to optimize for the most probable configuration
- The main difference between VO and SLAM is that VO mainly focuses on local consistency and aims to incrementally estimate the path of the camera/robot pose after pose, and possibly performing local optimization. Whereas SLAM aims to obtain a globally consistent estimate of the camera/robot trajectory and map. [An Overview to Visual Odometry and Visual SLAM: Applications to Mobile Robotics](https://link.springer.com/article/10.1007/s40903-015-0032-7)



### Relevant links
- [Competition GitHub README](https://github.com/hemi86/hiltislamchallenge)


### Doubts and resolutions
- `!!opencv-matrix` and key-value `dt: d` - [How do I write into YML format and read it with opencv and C++ ?](https://answers.opencv.org/question/226967/how-do-i-write-into-yml-format-and-read-it-with-opencv-and-c/)
- [Reading STEP file online](https://beta.sharecad.org/en/)
